{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ca342c",
   "metadata": {},
   "source": [
    "<b>Imports</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac75318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16190dec",
   "metadata": {},
   "source": [
    "<b>Settings</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(threshold=None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
    "pd.set_option(\"display.max_colwidth\", 99999)\n",
    "%matplotlib inline\n",
    "display(HTML(\"<style>.container { width: 90% !important; }</style>\"))\n",
    "\n",
    "N_JOBS = -1\n",
    "SEED = 2021\n",
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94485ec",
   "metadata": {},
   "source": [
    "<b>Functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beadfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimBayes(object):\n",
    "    def __init__(self,\n",
    "                 train,\n",
    "                 y_train,\n",
    "                 kind,\n",
    "                 n_estimators):\n",
    "        self.train=train\n",
    "        self.y_train=y_train\n",
    "        self.kind=kind\n",
    "        self.n_estimators=n_estimators\n",
    "\n",
    "    ###########################################################################################\n",
    "\n",
    "    def init_bayes_opt(self, model_name, min_max_params):\n",
    "        if (model_name == \"rf\"):\n",
    "            bo = BayesianOptimization(\n",
    "                f=self.run_rf,\n",
    "                pbounds={\n",
    "                    \"max_depth\": (min_max_params[\"max_depth_min\"],\n",
    "                                  min_max_params[\"max_depth_max\"]),\n",
    "                    \"max_features\": (min_max_params[\"max_features_min\"],\n",
    "                                     min_max_params[\"max_features_max\"]),\n",
    "                    \"min_samples_leaf\":\n",
    "                    (min_max_params[\"min_samples_leaf_min\"],\n",
    "                     min_max_params[\"min_samples_leaf_max\"]),\n",
    "                    \"min_samples_split\":\n",
    "                    (min_max_params[\"min_samples_split_min\"],\n",
    "                     min_max_params[\"min_samples_split_max\"]),\n",
    "                },\n",
    "                random_state=SEED)\n",
    "        elif (model_name == \"et\"):\n",
    "            bo = BayesianOptimization(\n",
    "                f=self.run_et,\n",
    "                pbounds={\n",
    "                    \"max_depth\": (min_max_params[\"max_depth_min\"],\n",
    "                                  min_max_params[\"max_depth_max\"]),\n",
    "                    \"max_features\": (min_max_params[\"max_features_min\"],\n",
    "                                     min_max_params[\"max_features_max\"]),\n",
    "                    \"min_samples_leaf\":\n",
    "                    (min_max_params[\"min_samples_leaf_min\"],\n",
    "                     min_max_params[\"min_samples_leaf_max\"]),\n",
    "                    \"min_samples_split\":\n",
    "                    (min_max_params[\"min_samples_split_min\"],\n",
    "                     min_max_params[\"min_samples_split_max\"]),\n",
    "                },\n",
    "                random_state=SEED)\n",
    "        elif (model_name == \"xg\"):\n",
    "            bo = BayesianOptimization(\n",
    "                f=self.run_xg,\n",
    "                pbounds={\n",
    "                    \"colsample_bylevel\":\n",
    "                    (min_max_params[\"colsample_bylevel_min\"],\n",
    "                     min_max_params[\"colsample_bylevel_max\"]),\n",
    "                    \"colsample_bytree\":\n",
    "                    (min_max_params[\"colsample_bytree_min\"],\n",
    "                     min_max_params[\"colsample_bytree_max\"]),\n",
    "                    \"gamma\": (min_max_params[\"gamma_min\"],\n",
    "                              min_max_params[\"gamma_max\"]),\n",
    "                    \"max_depth\": (min_max_params[\"max_depth_min\"],\n",
    "                                  min_max_params[\"max_depth_max\"]),\n",
    "                    \"min_child_weight\":\n",
    "                    (min_max_params[\"min_child_weight_min\"],\n",
    "                     min_max_params[\"min_child_weight_max\"]),\n",
    "                    \"subsample\": (min_max_params[\"subsample_min\"],\n",
    "                                  min_max_params[\"subsample_max\"]),\n",
    "                },\n",
    "                random_state=SEED)\n",
    "        elif (model_name == \"lg\"):\n",
    "            bo = BayesianOptimization(\n",
    "                f=self.run_lg,\n",
    "                pbounds={\n",
    "                    \"colsample_bytree\":\n",
    "                    (min_max_params[\"colsample_bytree_min\"],\n",
    "                     min_max_params[\"colsample_bytree_max\"]),\n",
    "                    \"is_unbalance\": (min_max_params[\"is_unbalance_min\"],\n",
    "                                     min_max_params[\"is_unbalance_max\"]),\n",
    "                    \"max_depth\": (min_max_params[\"max_depth_min\"],\n",
    "                                  min_max_params[\"max_depth_max\"]),\n",
    "                    \"min_child_samples\":\n",
    "                    (min_max_params[\"min_child_samples_min\"],\n",
    "                     min_max_params[\"min_child_samples_max\"]),\n",
    "                    \"min_split_gain\": (min_max_params[\"min_split_gain_min\"],\n",
    "                                       min_max_params[\"min_split_gain_max\"]),\n",
    "                    \"subsample\": (min_max_params[\"subsample_min\"],\n",
    "                                  min_max_params[\"subsample_max\"]),\n",
    "                    \"subsample_freq\": (min_max_params[\"subsample_freq_min\"],\n",
    "                                       min_max_params[\"subsample_freq_max\"]),\n",
    "                },\n",
    "                random_state=SEED)\n",
    "\n",
    "        return (bo)\n",
    "\n",
    "    ###########################################################################################\n",
    "\n",
    "    def show_best_combos(self, bo, length=15):\n",
    "        results_df = pd.DataFrame()\n",
    "        for i in range(len(bo.res)):\n",
    "            for key in bo.res[i].keys():\n",
    "                if key == \"target\":\n",
    "                    target = bo.res[i][key]\n",
    "                elif key == \"params\":\n",
    "                    cur_df = pd.DataFrame(bo.res[i][key], index=[0])\n",
    "            cur_df[\"target\"] = target\n",
    "            cur_df[\"iter\"] = i\n",
    "            results_df = pd.concat([results_df, cur_df])\n",
    "        results_df.sort_values([\"target\"], ascending=False, inplace=True)\n",
    "        results_df.set_index(\"iter\", inplace=True)\n",
    "        display(results_df.head(length))\n",
    "        display(results_df.tail(length))\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    ###########################################################################################\n",
    "\n",
    "    def pair_plots(self, history_df, min_max_params, param1, param2):\n",
    "        _x, _y, _z = history_df[param1].values, history_df[\n",
    "            param2].values, history_df[\"target\"].values\n",
    "\n",
    "        # Set up a regular grid of interpolation points\n",
    "        param1min = min_max_params[param1 + \"_min\"]\n",
    "        param1max = min_max_params[param1 + \"_max\"]\n",
    "        param2min = min_max_params[param2 + \"_min\"]\n",
    "        param2max = min_max_params[param2 + \"_max\"]\n",
    "        xi, yi = np.linspace(param1min, param1max, 100), np.linspace(\n",
    "            param2min, param2max, 100)\n",
    "        xi, yi = np.meshgrid(xi, yi)\n",
    "\n",
    "        # Interpolate\n",
    "        rbf = scipy.interpolate.Rbf(\n",
    "            _x, _y, _z, function=\"multiquadric\", smooth=2)\n",
    "        zi = rbf(xi, yi)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(\n",
    "            zi,\n",
    "            cmap=\"plasma\",\n",
    "            aspect=(param1max - param1min) / (param2max - param2min),\n",
    "            vmin=_z.min(),\n",
    "            vmax=_z.max(),\n",
    "            origin=\"lower\",\n",
    "            extent=[param1min, param1max, param2min, param2max])\n",
    "        q = plt.scatter(_x, _y, c=_z, cmap=\"plasma\")\n",
    "        plt.colorbar(q)\n",
    "        plt.xlabel(param1)\n",
    "        plt.ylabel(param2)\n",
    "        plt.show(block=False)\n",
    "\n",
    "    ###########################################################################################\n",
    "\n",
    "    def fit_and_predict(self, model, model_name):\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        # Iterate on each fold\n",
    "        kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "        for j, (train_idx, valid_idx) in enumerate(kf.split(train, y)):\n",
    "            '''if j == 2:\n",
    "                break'''\n",
    "            \n",
    "            X_train = train.iloc[train_idx]\n",
    "            X_valid = train.iloc[valid_idx]\n",
    "\n",
    "            y_train = y.iloc[X_train.index]\n",
    "            y_valid = y.iloc[X_valid.index]                  \n",
    "\n",
    "            # OOF predictions\n",
    "            if \"xg\" in model_name:\n",
    "                #sample_weight = self.y_train.replace({1 : 1., 2 : 3., 3 : 4.})\n",
    "                #sample_weight_eval_set = self.y_valid.replace({1 : 1., 2 : 3., 3 : 4.})\n",
    "                model.fit(\n",
    "                    X_train.drop([\"season\", \"name\"], axis=1),\n",
    "                    y_train,\n",
    "                    eval_set=[(X_valid.drop([\"season\", \"name\"], axis=1), y_valid)],\n",
    "                    early_stopping_rounds=10,\n",
    "                    verbose=0)\n",
    "                preds_X_valid = model.predict(\n",
    "                    X_valid.drop([\"season\", \"name\"], axis=1), ntree_limit=model.best_ntree_limit)\n",
    "                best_iteration = model.best_ntree_limit\n",
    "            elif \"lg\" in model_name:\n",
    "                model.fit(\n",
    "                    X_train.drop([\"season\", \"name\"], axis=1),\n",
    "                    y_train,\n",
    "                    eval_set=[(X_valid.drop([\"season\", \"name\"], axis=1), y_valid)],\n",
    "                    early_stopping_rounds=10,\n",
    "                    verbose=0)\n",
    "                preds_X_valid = model.predict(\n",
    "                    X_valid.drop([\"season\", \"name\"], axis=1), ntree_limit=model.best_iteration_)\n",
    "                best_iteration = model.best_iteration_\n",
    "            else:\n",
    "                model.fit(X_train.drop([\"season\", \"name\"], axis=1), y_train)\n",
    "                preds_X_valid = model.predict(X_valid.drop([\"season\", \"name\"], axis=1))\n",
    "                best_iteration = -1\n",
    "\n",
    "            all_preds.extend(preds_X_valid)\n",
    "            all_targets.extend(y_valid.values)\n",
    "\n",
    "        return (all_preds, all_targets)\n",
    "    \n",
    "    ###########################################################################################\n",
    "\n",
    "    def run_model(self, model, model_name):\n",
    "        # Fit and predict\n",
    "        all_preds, all_targets = self.fit_and_predict(model, model_name)\n",
    "\n",
    "        # Post process\n",
    "        postpro_df = pd.DataFrame()\n",
    "        postpro_df[\"targets\"] = all_targets\n",
    "        postpro_df[\"preds\"] = all_preds\n",
    "        postpro_df.loc[postpro_df[\"preds\"] < 0, \"preds\"] = 0\n",
    "\n",
    "        # Compute and return error\n",
    "        score = scoring(postpro_df[\"targets\"], postpro_df[\"preds\"])\n",
    "        return (score)\n",
    "\n",
    "    ###########################################################################################\n",
    "\n",
    "    def run_rf(self,\n",
    "                   max_features,\n",
    "                   max_depth,\n",
    "                   min_samples_split,\n",
    "                   min_samples_leaf):\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_features=max_features,\n",
    "            max_depth=int(max_depth),\n",
    "            min_samples_split=int(min_samples_split),\n",
    "            min_samples_leaf=int(min_samples_leaf),\n",
    "            random_state=SEED,\n",
    "            n_jobs=N_JOBS)\n",
    "        return (self.run_model(rf, \"rf\"))\n",
    "\n",
    "    ###########################################################################################\n",
    "\n",
    "    def run_et(self, \n",
    "                   max_features, \n",
    "                   max_depth, \n",
    "                   min_samples_split,\n",
    "                   min_samples_leaf):\n",
    "        et = ExtraTreesRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_features=max_features,\n",
    "            max_depth=int(max_depth),\n",
    "            min_samples_split=int(min_samples_split),\n",
    "            min_samples_leaf=int(min_samples_leaf),\n",
    "            random_state=SEED,\n",
    "            n_jobs=N_JOBS)\n",
    "\n",
    "        return (self.run_model(et, \"et\"))\n",
    "\n",
    "    ###########################################################################################\n",
    "\n",
    "    def run_xg(self, max_depth, min_child_weight, subsample,\n",
    "                   colsample_bytree, colsample_bylevel, gamma):\n",
    "        xg = XGBRegressor(\n",
    "            objective=\"multi:softmax\",\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=int(max_depth),\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            colsample_bylevel=colsample_bylevel,\n",
    "            gamma=gamma,\n",
    "            seed=SEED,\n",
    "            nthread=N_JOBS)\n",
    "\n",
    "        return (self.run_model(xg, \"xg\"))\n",
    "\n",
    "    ###########################################################################################\n",
    "\n",
    "    def run_lg(self, max_depth, min_child_samples, subsample,\n",
    "                   subsample_freq, colsample_bytree, min_split_gain, is_unbalance):\n",
    "        num_leaves = (2**int(max_depth)) - 1\n",
    "        if (num_leaves > 4095):\n",
    "            num_leaves = 4095\n",
    "            \n",
    "        if (self.kind == \"classification\"):\n",
    "            if (int(is_unbalance) > 1):\n",
    "                apply_weights = \"balanced\"\n",
    "            else:\n",
    "                apply_weights = None\n",
    "\n",
    "        lg = LGBMRegressor(\n",
    "            boosting_type=\"gbdt\",\n",
    "            objective=\"regression\",\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=self.n_estimators,\n",
    "            num_leaves=num_leaves,\n",
    "            max_depth=int(max_depth),\n",
    "            min_child_samples=int(min_child_samples),\n",
    "            subsample=subsample,\n",
    "            subsample_freq=int(subsample_freq),\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            min_split_gain=min_split_gain,\n",
    "            seed=SEED,\n",
    "            n_jobs=N_JOBS)\n",
    "\n",
    "        return (self.run_model(lg, \"lg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.core.series.Series):\n",
    "        mask = y_true.notnull()\n",
    "        y_true = y_true[mask].tolist()\n",
    "        y_pred = y_pred[mask].tolist()\n",
    "    elif isinstance(y_pred, np.ndarray):\n",
    "        negmask = np.isnan(y_true)\n",
    "        y_true = y_true[~negmask]\n",
    "        y_pred = y_pred[~negmask]   \n",
    "    \n",
    "    return -np.round(mean_squared_error(y_true, y_pred), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca5af7",
   "metadata": {},
   "source": [
    "<b>Script</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "init_train = pd.read_csv(\"../data/processed/all_seasons.csv\")\n",
    "\n",
    "# Use all seasons but 2018 (the one we try to predict) and 2017 (which will be used for prediction)\n",
    "print(init_train.shape[0])\n",
    "init_train = init_train.loc[(init_train[\"season\"] != 2018) & (init_train[\"season\"] != 2017), :]\n",
    "print(init_train.shape[0])\n",
    "display(init_train.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38352f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply initial config\n",
    "y = init_train[\"playoff_wins\"]\n",
    "train = init_train.drop([\"playoff_wins\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec6704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters ranges and initial exploration space\n",
    "min_max_params_rf = {\n",
    "    \"max_depth_min\" : 4,\n",
    "    \"max_depth_max\" : 12,\n",
    "    \"max_features_min\" : 0.2,\n",
    "    \"max_features_max\" : 0.9,\n",
    "    \"min_samples_leaf_min\" : 2,\n",
    "    \"min_samples_leaf_max\" : 10,\n",
    "    \"min_samples_split_min\" : 2,\n",
    "    \"min_samples_split_max\" : 10,\n",
    "}\n",
    "\n",
    "min_max_params_xg = {\n",
    "    \"colsample_bylevel_min\" : 0.3,\n",
    "    \"colsample_bylevel_max\" : 1.0,\n",
    "    \"colsample_bytree_min\" : 0.3,\n",
    "    \"colsample_bytree_max\" : 1.0,\n",
    "    \"gamma_min\" : 1,\n",
    "    \"gamma_max\" : 50,\n",
    "    \"max_depth_min\" : 5,\n",
    "    \"max_depth_max\" : 15,\n",
    "    \"min_child_weight_min\" : 2,\n",
    "    \"min_child_weight_max\" : 30,\n",
    "    \"subsample_min\" : 0.3,\n",
    "    \"subsample_max\" : 1.0,\n",
    "}\n",
    "\n",
    "min_max_params_lg = {\n",
    "    \"colsample_bytree_min\" : 0.25,\n",
    "    \"colsample_bytree_max\" : 1.0,\n",
    "    \"is_unbalance_min\" : 1,\n",
    "    \"is_unbalance_max\" : 1.9,\n",
    "    \"max_depth_min\" : 4,\n",
    "    \"max_depth_max\" : 20,\n",
    "    \"min_child_samples_min\" : 10,\n",
    "    \"min_child_samples_max\" : 25,\n",
    "    \"min_split_gain_min\" : 0.001,\n",
    "    \"min_split_gain_max\" : 0.01,\n",
    "    \"subsample_min\" : 0.2,\n",
    "    \"subsample_max\" : 0.75,\n",
    "    \"subsample_freq_min\" : 1,\n",
    "    \"subsample_freq_max\" : 8,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "problem_type = \"regression\"\n",
    "model_name = \"lg\"\n",
    "n_estimators = 50\n",
    "\n",
    "print(\"MODEL : \" + model_name)\n",
    "o_b = OptimBayes(\n",
    "    train, \n",
    "    y, \n",
    "    problem_type, \n",
    "    n_estimators=n_estimators)\n",
    "bo = o_b.init_bayes_opt(model_name, min_max_params_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1049e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Maximization\n",
    "init_points = 100\n",
    "n_iter = 300\n",
    "xi = 0.07 # between 0.0 (exploitation) and 0.1 (exploration)\n",
    "\n",
    "start = time.time()\n",
    "bo.maximize(init_points=init_points, n_iter=n_iter, xi=xi, acq=\"ei\")\n",
    "print(\"BayesianOptimization took %.2f seconds\" % ((time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b163d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show results\n",
    "history_df = o_b.show_best_combos(bo, length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ebfbf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw pair plots\n",
    "if (model_name == \"xg\") :\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"max_depth\", \"min_child_weight\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"max_depth\", \"subsample\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"max_depth\", \"colsample_bytree\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"max_depth\", \"colsample_bylevel\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"max_depth\", \"gamma\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"min_child_weight\", \"subsample\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"min_child_weight\", \"colsample_bytree\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"min_child_weight\", \"colsample_bylevel\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"min_child_weight\", \"gamma\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"subsample\", \"colsample_bytree\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"subsample\", \"colsample_bylevel\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"subsample\", \"gamma\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"colsample_bytree\", \"colsample_bylevel\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"colsample_bytree\", \"gamma\")\n",
    "    o_b.pair_plots(history_df, min_max_params_xg, \"colsample_bylevel\", \"gamma\")\n",
    "elif (model_name == \"lg\") :\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"max_depth\", \"min_child_samples\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"max_depth\", \"subsample\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"max_depth\", \"subsample_freq\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"max_depth\", \"colsample_bytree\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"max_depth\", \"min_split_gain\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"max_depth\", \"is_unbalance\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"min_child_samples\", \"subsample\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"min_child_samples\", \"subsample_freq\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"min_child_samples\", \"colsample_bytree\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"min_child_samples\", \"min_split_gain\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"min_child_samples\", \"is_unbalance\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"subsample\", \"subsample_freq\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"subsample\", \"colsample_bytree\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"subsample\", \"min_split_gain\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"subsample\", \"is_unbalance\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"subsample_freq\", \"colsample_bytree\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"subsample_freq\", \"min_split_gain\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"subsample_freq\", \"is_unbalance\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"colsample_bytree\", \"min_split_gain\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"colsample_bytree\", \"is_unbalance\")\n",
    "    o_b.pair_plots(history_df, min_max_params_lg, \"min_split_gain\", \"is_unbalance\")\n",
    "elif (model_name == \"et\") :\n",
    "    o_b.pair_plots(history_df, min_max_params_et, \"max_depth\", \"max_features\")\n",
    "    o_b.pair_plots(history_df, min_max_params_et, \"max_depth\", \"min_samples_leaf\")\n",
    "    o_b.pair_plots(history_df, min_max_params_et, \"max_depth\", \"min_samples_split\")\n",
    "    o_b.pair_plots(history_df, min_max_params_et, \"min_samples_leaf\", \"min_samples_split\")\n",
    "elif (model_name == \"rf\") :\n",
    "    o_b.pair_plots(history_df, min_max_params_rf, \"max_depth\", \"max_features\")\n",
    "    o_b.pair_plots(history_df, min_max_params_rf, \"max_depth\", \"min_samples_leaf\")\n",
    "    o_b.pair_plots(history_df, min_max_params_rf, \"max_depth\", \"min_samples_split\")\n",
    "    o_b.pair_plots(history_df, min_max_params_rf, \"min_samples_leaf\", \"min_samples_split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a25581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbde79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
